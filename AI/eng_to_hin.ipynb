{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf72842-8560-4473-a968-87bbf7296065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1497441d-2a2e-4677-81d6-bf9f24e58f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting align\n",
      "  Using cached align-0.1.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /opt/anaconda3/lib/python3.11/site-packages (from align) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /opt/anaconda3/lib/python3.11/site-packages (from align) (1.13.0)\n",
      "Requirement already satisfied: gensim>=4.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from align) (4.3.0)\n",
      "Requirement already satisfied: pandas>=1.4.2 in /opt/anaconda3/lib/python3.11/site-packages (from align) (2.2.1)\n",
      "Requirement already satisfied: nltk>=3.7 in /opt/anaconda3/lib/python3.11/site-packages (from align) (3.8.1)\n",
      "Requirement already satisfied: ipython>=8.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from align) (8.20.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from gensim>=4.1.2->align) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim>=4.1.2->align)\n",
      "  Using cached FuzzyTM-2.0.9-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.11/site-packages (from ipython>=8.3.0->align) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.11/site-packages (from ipython>=8.3.0->align) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.11/site-packages (from ipython>=8.3.0->align) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.11/site-packages (from ipython>=8.3.0->align) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from ipython>=8.3.0->align) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.11/site-packages (from ipython>=8.3.0->align) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in /opt/anaconda3/lib/python3.11/site-packages (from ipython>=8.3.0->align) (5.7.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.11/site-packages (from ipython>=8.3.0->align) (4.8.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk>=3.7->align) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk>=3.7->align) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk>=3.7->align) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk>=3.7->align) (4.65.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=1.4.2->align) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=1.4.2->align) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=1.4.2->align) (2023.3)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim>=4.1.2->align)\n",
      "  Using cached pyFUME-0.3.4-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from jedi>=0.16->ipython>=8.3.0->align) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.11/site-packages (from pexpect>4.3->ipython>=8.3.0->align) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=8.3.0->align) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.2->align) (1.16.0)\n",
      "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=8.3.0->align) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=8.3.0->align) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=8.3.0->align) (0.2.2)\n",
      "Collecting scipy>=1.7.3 (from align)\n",
      "  Using cached scipy-1.10.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (100 kB)\n",
      "Collecting numpy>=1.22.3 (from align)\n",
      "  Using cached numpy-1.24.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Collecting simpful==2.12.0 (from pyfume->FuzzyTM>=0.4.0->gensim>=4.1.2->align)\n",
      "  Using cached simpful-2.12.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting fst-pso==1.8.1 (from pyfume->FuzzyTM>=0.4.0->gensim>=4.1.2->align)\n",
      "  Using cached fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pandas>=1.4.2 (from align)\n",
      "  Using cached pandas-1.5.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting miniful (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim>=4.1.2->align)\n",
      "  Using cached miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hUsing cached align-0.1.1-py3-none-any.whl (2.5 MB)\n",
      "Using cached FuzzyTM-2.0.9-py3-none-any.whl (31 kB)\n",
      "Using cached pyFUME-0.3.4-py3-none-any.whl (60 kB)\n",
      "Using cached numpy-1.24.4-cp311-cp311-macosx_11_0_arm64.whl (13.8 MB)\n",
      "Using cached pandas-1.5.3-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "Downloading scipy-1.10.1-cp311-cp311-macosx_12_0_arm64.whl (28.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.7/28.7 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading simpful-2.12.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20430 sha256=cb7115b8a6a3dc60230331cd8a953577d1d5333c82c6963e1e2a16201889ad0e\n",
      "  Stored in directory: /Users/nischalchand/Library/Caches/pip/wheels/69/f5/e5/18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
      "  Building wheel for miniful (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3507 sha256=f6eca4c1757bca18e28ffd2149699d70c2c558ebd53d64a592f1444a1847e9fd\n",
      "  Stored in directory: /Users/nischalchand/Library/Caches/pip/wheels/9d/ff/2f/afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: numpy, scipy, pandas, simpful, miniful, fst-pso, pyfume, FuzzyTM, align\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.13.0\n",
      "    Uninstalling scipy-1.13.0:\n",
      "      Successfully uninstalled scipy-1.13.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.1\n",
      "    Uninstalling pandas-2.2.1:\n",
      "      Successfully uninstalled pandas-2.2.1\n",
      "Successfully installed FuzzyTM-2.0.9 align-0.1.1 fst-pso-1.8.1 miniful-0.0.6 numpy-1.24.4 pandas-1.5.3 pyfume-0.3.4 scipy-1.10.1 simpful-2.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c501c143-4e82-4441-afbc-eb569d8de70f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'alignment' from 'align' (/opt/anaconda3/lib/python3.11/site-packages/align/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malign\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alignment\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malign\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IBMModel1\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# English and Hindi sentence pairs\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'alignment' from 'align' (/opt/anaconda3/lib/python3.11/site-packages/align/__init__.py)"
     ]
    }
   ],
   "source": [
    "from align import alignment\n",
    "from align.models import IBMModel1\n",
    "\n",
    "# English and Hindi sentence pairs\n",
    "english_sentences = [\"he goes to school\", \"she reads a book\"]\n",
    "hindi_sentences = [\"वह स्कूल जाता है\", \"वह किताब पढ़ती है\"]\n",
    "\n",
    "# Tokenize sentences into words\n",
    "english_tokens = [sentence.split() for sentence in english_sentences]\n",
    "hindi_tokens = [sentence.split() for sentence in hindi_sentences]\n",
    "\n",
    "# Train IBM Model 1 for word alignment\n",
    "ibm_model = IBMModel1(english_tokens, hindi_tokens)\n",
    "ibm_model.train()\n",
    "\n",
    "# Perform word alignment for each sentence pair\n",
    "for i in range(len(english_sentences)):\n",
    "    alignment_indices = alignment.align(ibm_model, english_tokens[i], hindi_tokens[i])\n",
    "    alignment_pairs = [(english_tokens[i][e], hindi_tokens[i][h]) for e, h in alignment_indices]\n",
    "    print(f\"English Sentence: {' '.join(english_tokens[i])}\")\n",
    "    print(f\"Hindi Sentence: {' '.join(hindi_tokens[i])}\")\n",
    "    print(\"Word Alignment:\")\n",
    "    for pair in alignment_pairs:\n",
    "        print(f\"{pair[0]} - {pair[1]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40884f4f-566d-4afd-9cf3-8897971121ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'IBMModel1' from 'nltk.translate.ibm_model' (/opt/anaconda3/lib/python3.11/site-packages/nltk/translate/ibm_model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Alignment, AlignedSent\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mibm_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IBMModel1\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# English and Hindi sentence pairs\u001b[39;00m\n\u001b[1;32m      6\u001b[0m english_sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhe goes to school\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshe reads a book\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'IBMModel1' from 'nltk.translate.ibm_model' (/opt/anaconda3/lib/python3.11/site-packages/nltk/translate/ibm_model.py)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate import Alignment, AlignedSent\n",
    "from nltk.translate.ibm_model import IBMModel1\n",
    "\n",
    "# English and Hindi sentence pairs\n",
    "english_sentences = [\"he goes to school\", \"she reads a book\"]\n",
    "hindi_sentences = [\"वह स्कूल जाता है\", \"वह किताब पढ़ती है\"]\n",
    "\n",
    "# Tokenize sentences into words\n",
    "english_tokens = [nltk.word_tokenize(sentence) for sentence in english_sentences]\n",
    "hindi_tokens = [nltk.word_tokenize(sentence) for sentence in hindi_sentences]\n",
    "\n",
    "# Create AlignedSent objects\n",
    "aligned_sents = [AlignedSent(english, hindi) for english, hindi in zip(english_tokens, hindi_tokens)]\n",
    "\n",
    "# Train IBM Model 1 for word alignment\n",
    "ibm_model = IBMModel1(aligned_sents)\n",
    "\n",
    "# Perform word alignment for each sentence pair\n",
    "for i in range(len(english_sentences)):\n",
    "    alignment = ibm_model.align(AlignedSent(english_tokens[i], hindi_tokens[i]))\n",
    "    print(f\"English Sentence: {' '.join(english_tokens[i])}\")\n",
    "    print(f\"Hindi Sentence: {' '.join(hindi_tokens[i])}\")\n",
    "    print(\"Word Alignment:\")\n",
    "    for e, h in alignment:\n",
    "        print(f\"{english_tokens[i][e]} - {hindi_tokens[i][h]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cfd9b45-8f2a-4678-8236-37a9cfb62045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60332909-7c5f-4db0-97aa-0a6da7c5cfa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Alignment' has no attribute 'from_strings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Perform word alignment for each sentence pair\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(english_sentences)):\n\u001b[0;32m---> 13\u001b[0m     alignment \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtranslate\u001b[38;5;241m.\u001b[39mAlignment\u001b[38;5;241m.\u001b[39mfrom_strings(english_tokens[i], hindi_tokens[i])\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish Sentence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(english_tokens[i])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHindi Sentence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(hindi_tokens[i])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Alignment' has no attribute 'from_strings'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# English and Hindi sentence pairs\n",
    "english_sentences = [\"he goes to school\", \"she reads a book\"]\n",
    "hindi_sentences = [\"वह स्कूल जाता है\", \"वह किताब पढ़ती है\"]\n",
    "\n",
    "# Tokenize sentences into words\n",
    "english_tokens = [nltk.word_tokenize(sentence) for sentence in english_sentences]\n",
    "hindi_tokens = [nltk.word_tokenize(sentence) for sentence in hindi_sentences]\n",
    "\n",
    "# Perform word alignment for each sentence pair\n",
    "for i in range(len(english_sentences)):\n",
    "    alignment = nltk.translate.Alignment.from_strings(english_tokens[i], hindi_tokens[i])\n",
    "    print(f\"English Sentence: {' '.join(english_tokens[i])}\")\n",
    "    print(f\"Hindi Sentence: {' '.join(hindi_tokens[i])}\")\n",
    "    print(\"Word Alignment:\")\n",
    "    for e, h in alignment:\n",
    "        print(f\"{english_tokens[i][e]} - {hindi_tokens[i][h]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5d04193-553d-4f61-8663-ec57afc20d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence: he goes to school\n",
      "Hindi Sentence: वह स्कूल जाता है\n",
      "Word Alignment:\n",
      "he - वह\n",
      "goes - स्कूल\n",
      "to - जाता\n",
      "school - है\n",
      "\n",
      "English Sentence: she reads a book\n",
      "Hindi Sentence: वह किताब पढ़ती है\n",
      "Word Alignment:\n",
      "she - वह\n",
      "reads - किताब\n",
      "a - पढ़ती\n",
      "book - है\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# English and Hindi sentence pairs\n",
    "english_sentences = [\"he goes to school\", \"she reads a book\"]\n",
    "hindi_sentences = [\"वह स्कूल जाता है\", \"वह किताब पढ़ती है\"]\n",
    "\n",
    "# Tokenize sentences into words\n",
    "english_tokens = [nltk.word_tokenize(sentence) for sentence in english_sentences]\n",
    "hindi_tokens = [nltk.word_tokenize(sentence) for sentence in hindi_sentences]\n",
    "\n",
    "# Perform word alignment for each sentence pair\n",
    "for i in range(len(english_sentences)):\n",
    "    alignment = list(zip(range(len(english_tokens[i])), range(len(hindi_tokens[i]))))\n",
    "    print(f\"English Sentence: {' '.join(english_tokens[i])}\")\n",
    "    print(f\"Hindi Sentence: {' '.join(hindi_tokens[i])}\")\n",
    "    print(\"Word Alignment:\")\n",
    "    for e, h in alignment:\n",
    "        print(f\"{english_tokens[i][e]} - {hindi_tokens[i][h]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c00a99-bb41-4b95-b889-6cee8001934b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
